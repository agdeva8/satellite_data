{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deva/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named reportlab.pdfgen",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2eb8bb7c7790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margrelextrema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreportlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfgen\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named reportlab.pdfgen"
     ]
    }
   ],
   "source": [
    "### Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "%pylab\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.signal import argrelextrema\n",
    "from reportlab.pdfgen import canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function to Apply RFE\n",
    "def featureRank(df, y):\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    selector = RFE(model,1, step=1)\n",
    "    \n",
    "    X = df.values\n",
    "    X = preprocessing.scale(X)\n",
    "    \n",
    "    selector.fit(X,y)\n",
    "    selectedIdx = selector.get_support(indices=True)\n",
    "    featureRanking = selector.ranking_\n",
    "    var=list(df)\n",
    "    ranked_var = pd.DataFrame(featureRanking,var)\n",
    "    ranked_var = ranked_var.reset_index()\n",
    "    ranked_var.columns = (['var','rank'])\n",
    "    ranked_var = ranked_var.sort_values('rank')\n",
    "    ranked_var.reset_index(inplace=True)\n",
    "    del ranked_var['index']\n",
    "    \n",
    "    return ranked_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selectFeatures(df,ranked_var,y):\n",
    "    metric_list = list()\n",
    "    f1_list = list()\n",
    "    predic_list =list()\n",
    "    for i in range(len(ranked_var)):\n",
    "        var2 = ranked_var.loc[:i,'var'].values.tolist()\n",
    "\n",
    "        X = df.loc[:,var2].values\n",
    "\n",
    "        model = LogisticRegression(class_weight='balanced')\n",
    "        clf = make_pipeline(preprocessing.StandardScaler(),model)\n",
    "        predicted = cross_validation.cross_val_predict(clf,X,y, cv=5)\n",
    "\n",
    "        acc = metrics.accuracy_score(y,predicted)\n",
    "        recall = metrics.recall_score(y,predicted)\n",
    "        prec =  metrics.precision_score(y,predicted)\n",
    "\n",
    "        cm = metrics.confusion_matrix(y, predicted)\n",
    "        cm_score = float(cm[1][1])/(cm[1][1]+cm[1][0]+cm[0][1])\n",
    "\n",
    "        f1 = metrics.f1_score(y,predicted)\n",
    "\n",
    "        metric_list.append([f1,cm_score,acc,recall,prec])\n",
    "        f1_list.append(f1)\n",
    "        predic_list.append(predicted)\n",
    "\n",
    "    df_metric = pd.DataFrame(metric_list)\n",
    "    df_metric.columns =  (['f1','cm','acc','recall','prec'])\n",
    "\n",
    "    ## finding the global maximum index in f1_list\n",
    "    global_maxima = f1_list.index(max(f1_list))\n",
    "\n",
    "    ## finding indexes of all local maximums\n",
    "\n",
    "    x = array(f1_list)\n",
    "    # for local maxima\n",
    "    local_maxima = argrelextrema(x, np.greater)[0].tolist()\n",
    "\n",
    "    f1_acceptable = f1_list[global_maxima]-0.01\n",
    "\n",
    "    i = 0\n",
    "    while f1_list[i] < f1_acceptable:\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "    var_final = ranked_var.loc[:i,'var']\n",
    "    df_modelPerformance=df_metric.loc[i,:]\n",
    "\n",
    "    X = df.loc[:,var_final].values\n",
    "\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    fit = model.fit(X,y)\n",
    "    predicted = predic_list[i]\n",
    "\n",
    "    df_selecedVar = ranked_var.loc[:i].copy(deep=True)\n",
    "    df_selecedVar.insert(2,'coeff',fit.coef_[0])\n",
    "    \n",
    "\n",
    "    return df_selecedVar,df_modelPerformance, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Data Files\n",
    "df_satData = pd.read_csv('C:/Users/Night Fury/IPYNB/IITD-Project/20180601_DistSatellite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### List of labels\n",
    "lis_labelsAll = ['MSL_1','MSL_2','MSL_3','MSW_1','MSW_2','MSW_3','CHH_1','CHH_2','CHH_3','FC_1','FC_2','FC_3','BF_1','BF_2','BF_3','EMP_1','EMP_2','EMP_3']\n",
    "\n",
    "### List of useful Labels\n",
    "lis_labels= ['EMP_1','EMP_2','EMP_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Building features from night light data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Raw NTL Variable list\n",
    "var = []\n",
    "for i in range(64):\n",
    "    var.append('DN_' + str(i).zfill(2))\n",
    "\n",
    "### Sum of all pixels' Area    \n",
    "df_satData['Area'] = 0\n",
    "for i in var:\n",
    "    df_satData['Area']=df_satData['Area'] + df_satData[i]\n",
    "df_satData['Area'] = df_satData['Area'].replace(0,1)\n",
    "\n",
    "### Pixels' Area / Total Area \n",
    "for i in var:\n",
    "    df_satData['p'+i] = (df_satData[i])/(df_satData['Area']*1.0)\n",
    "\n",
    "### Pixels' Area weighted by Pixels' Strength\n",
    "for i,j in enumerate(var):\n",
    "    df_satData['q'+j] = df_satData[j]*(i+1)\n",
    "\n",
    "### Useful variable list\n",
    "var3 = []\n",
    "for i in var:\n",
    "    var3.append('p'+i)\n",
    "for i in var:\n",
    "    var3.append('q'+i)\n",
    "#for i in var:\n",
    "#    var3.append(i)\n",
    "#for i in var:\n",
    "#    var1.append('r'+i)\n",
    "#for i in var:\n",
    "#    var1.append('rp'+i)\n",
    "#for i in var:\n",
    "#    var1.append('rq'+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appling PCA to fetures formed from nightlight pixels' area data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 42.46  59.22  67.13  71.43  74.91  78.13  80.38  82.53  84.4   85.9\n",
      "  87.11  88.15  89.08  89.86  90.5   91.1   91.68  92.12  92.54  92.95\n",
      "  93.31  93.63  93.93  94.21  94.48  94.73  94.96  95.18  95.38  95.58\n",
      "  95.77  95.95  96.12  96.29  96.45  96.61  96.75  96.89  97.03  97.16\n",
      "  97.29  97.41  97.52  97.63  97.74  97.84  97.94  98.04  98.14  98.23\n",
      "  98.32  98.41  98.49  98.57  98.65  98.72  98.79  98.86  98.93  98.99\n",
      "  99.05  99.11  99.17  99.22  99.27  99.32  99.37  99.41  99.45  99.49\n",
      "  99.52  99.55  99.58  99.61  99.64  99.67  99.69  99.71  99.73  99.75\n",
      "  99.77  99.79  99.8   99.81  99.82  99.83  99.84  99.85  99.86  99.87\n",
      "  99.88  99.89  99.9   99.91  99.92  99.93  99.94  99.95  99.96  99.97\n",
      "  99.98  99.98  99.98  99.98  99.98  99.98  99.98  99.98  99.98  99.98\n",
      "  99.98  99.98  99.98  99.98  99.98  99.98  99.98  99.98  99.98  99.98\n",
      "  99.98  99.98  99.98  99.98  99.98  99.98  99.98  99.98]\n",
      "n: 95% Variance:  10\n"
     ]
    }
   ],
   "source": [
    "X = df_satData.loc[:,var3].values\n",
    "X = preprocessing.scale(X)\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "#pca= PCA(svd_solver='full')\n",
    "#pca.fit(X, y=None )\n",
    "\n",
    "#The amount of variance that each PC explains\n",
    "var= pca.explained_variance_ratio_\n",
    "\n",
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "print(var1)\n",
    "\n",
    "#Number of PC comprising 95%Variance\n",
    "n = len(var1)-len([num for num in var1 if num != 0 and num > 85.0])+1\n",
    "print( \"n: 95% Variance: \",n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA fit\n",
    "X=pca.fit_transform(X)\n",
    "X=X[:,0:n]\n",
    "\n",
    "### Creating Dataframe of PC\n",
    "col_X = []\n",
    "for j in range(n):\n",
    "    col_X.append('PCA_'+str(j))\n",
    "df_pcaNTL = pd.DataFrame(X,columns=col_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building features out of Modis Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Developing features\n",
    "\n",
    "df_satData['Urban and built-up']=df_satData['Urban and built-up']+1\n",
    "df_satData['CropRatio']=(df_satData['Croplands']+df_satData['Cropland/Natural vegetation mosaic'])/df_satData['Area']\n",
    "df_satData['AvgUrbanNTL']=df_satData['sum']/df_satData['Urban and built-up']\n",
    "df_satData['UrbanRatio']= df_satData['Urban and built-up']/df_satData['Area']\n",
    "\n",
    "df_satData['Natural']= 0\n",
    "for i in ['Water','Evergreen Needleleaf forest','Evergreen Broadleaf forest','Deciduous Needleleaf forest','Deciduous Broadleaf forest','Mixed forest','Closed shrublands','Open shrublands','Woody savannas','Savannas','Grasslands','Permanent wetlands','Snow and ice']:\n",
    "    df_satData['Natural']=df_satData['Natural']+df_satData[i]\n",
    "    \n",
    "df_satData['CropRemainRatio']=(df_satData['Croplands']+df_satData['Cropland/Natural vegetation mosaic'])/(df_satData['Area']-df_satData['Natural'])\n",
    "df_satData['UrbanRemainRatio']=df_satData['Urban and built-up']/(df_satData['Area']-df_satData['Natural'])\n",
    "#df_satData['UrbanRatio']=df_satData['UrbanRatio'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Developing list of features\n",
    "\n",
    "modi_var=['mean','sum','CropRatio', 'AvgUrbanNTL', 'UrbanRatio', 'CropRemainRatio', 'UrbanRemainRatio','Natural']\n",
    "for i in ['Water',\n",
    " 'Evergreen Needleleaf forest',\n",
    " 'Evergreen Broadleaf forest',\n",
    " 'Deciduous Needleleaf forest',\n",
    " 'Deciduous Broadleaf forest',\n",
    " 'Mixed forest',\n",
    " 'Closed shrublands',\n",
    " 'Open shrublands',\n",
    " 'Woody savannas',\n",
    " 'Savannas',\n",
    " 'Grasslands',\n",
    " 'Permanent wetlands',\n",
    " 'Croplands',\n",
    " 'Urban and built-up',\n",
    " 'Cropland/Natural vegetation mosaic',\n",
    " 'Snow and ice']:\n",
    "    modi_var.append(i)\n",
    "    df_satData['mod_'+i]=df_satData[i]/df_satData['Area']\n",
    "    modi_var.append('mod_'+i)\n",
    "\n",
    "#for i in ['Water','Natural', 'Croplands', 'Urban and built-up', 'Cropland/Natural vegetation mosaic']:\n",
    " #   df_satData['mod_'+i]=df_satData[i]/df_satData['Area']\n",
    "#  modi_var.append('mod_'+i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final data from pca version of ntl and modis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DataFrame\n",
    "df_concatModNTL =  pd.concat([df_satData[modi_var],df_pcaNTL],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean',\n",
       " 'sum',\n",
       " 'CropRatio',\n",
       " 'AvgUrbanNTL',\n",
       " 'UrbanRatio',\n",
       " 'CropRemainRatio',\n",
       " 'UrbanRemainRatio',\n",
       " 'Water',\n",
       " 'mod_Water',\n",
       " 'Evergreen Needleleaf forest',\n",
       " 'mod_Evergreen Needleleaf forest',\n",
       " 'Evergreen Broadleaf forest',\n",
       " 'mod_Evergreen Broadleaf forest',\n",
       " 'Deciduous Needleleaf forest',\n",
       " 'mod_Deciduous Needleleaf forest',\n",
       " 'Deciduous Broadleaf forest',\n",
       " 'mod_Deciduous Broadleaf forest',\n",
       " 'Mixed forest',\n",
       " 'mod_Mixed forest',\n",
       " 'Closed shrublands',\n",
       " 'mod_Closed shrublands',\n",
       " 'Open shrublands',\n",
       " 'mod_Open shrublands',\n",
       " 'Woody savannas',\n",
       " 'mod_Woody savannas',\n",
       " 'Savannas',\n",
       " 'mod_Savannas',\n",
       " 'Grasslands',\n",
       " 'mod_Grasslands',\n",
       " 'Permanent wetlands',\n",
       " 'mod_Permanent wetlands',\n",
       " 'Croplands',\n",
       " 'mod_Croplands',\n",
       " 'Urban and built-up',\n",
       " 'mod_Urban and built-up',\n",
       " 'Cropland/Natural vegetation mosaic',\n",
       " 'mod_Cropland/Natural vegetation mosaic',\n",
       " 'Snow and ice',\n",
       " 'mod_Snow and ice',\n",
       " 'PCA_0',\n",
       " 'PCA_1',\n",
       " 'PCA_2',\n",
       " 'PCA_3',\n",
       " 'PCA_4',\n",
       " 'PCA_5',\n",
       " 'PCA_6',\n",
       " 'PCA_7',\n",
       " 'PCA_8',\n",
       " 'PCA_9']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_concatModNTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "df_selectedFeat=pd.DataFrame()\n",
    "df_modelPerf=pd.DataFrame()\n",
    "df_varPerf=pd.DataFrame()\n",
    "for i in ['FC','BF','MSL','MSW','EMP']:\n",
    "    lis_labels=[s for s in lis_labelsAll if i in s]\n",
    "    df_predicted=pd.DataFrame(columns=['True','Pred'])\n",
    "    for label in lis_labels:\n",
    "        y=df_satData.loc[:,label].values\n",
    "        ranked_var = featureRank(df_concatModNTL,y)\n",
    "        df_selectedFeatTemp, df_modelPerfTemp, predicted=selectFeatures(df_concatModNTL,ranked_var,y)\n",
    "        df_modelPerfTemp['label']=label\n",
    "        df_modelPerf=df_modelPerf.append(df_modelPerfTemp)\n",
    "        df_selectedFeatTemp['label']=label\n",
    "        df_selectedFeat=df_selectedFeat.append(df_selectedFeatTemp)\n",
    "        df_predictedTemp=pd.DataFrame(index= df_satData.index,columns=['True','Pred'])\n",
    "        df_predictedTemp['True']=y\n",
    "        df_predictedTemp['Pred']=predicted\n",
    "        df_predicted=df_predicted.append(df_predictedTemp)\n",
    "    df_varPerfTemp=pd.DataFrame(columns=['Var','ACC','F1'])\n",
    "    df_varPerfTemp.loc[0,'Var']=i\n",
    "    df_varPerfTemp.loc[0,'ACC']=metrics.accuracy_score(df_predicted['True'], df_predicted['Pred'])\n",
    "    df_varPerfTemp.loc[0,'F1']=metrics.f1_score(df_predicted['True'], df_predicted['Pred'])\n",
    "    df_varPerf=df_varPerf.append(df_varPerfTemp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelPerf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
